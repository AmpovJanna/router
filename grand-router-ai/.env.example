# Grand Router AI - Environment Configuration
#
# Copy to .env (and optionally .env.local) at the repo root.
# The backend will load .env then .env.local automatically on startup via
# [`load_env()`](grand-router-ai/backend/src/grand_router_api/services/settings/env.py:6).
#
# IMPORTANT:
# - Do NOT commit real secrets.
# - Frontend env vars MUST be prefixed with VITE_ (and are public in the browser).

############################################
# Backend (FastAPI)
############################################

# Persistence (where chats/messages are stored)
GRAND_ROUTER_PERSISTENCE_PATH=./backend/data

# Backend listen port (if your start script uses it)
PORT=8000

############################################
# LLM (OpenAI / OpenAI-compatible)
############################################

# Enable real LLM calls (defaults are stub)
CODEGEN_LLM_MODE=openai
ROUTER_LLM_MODE=openai

# OpenAI-compatible credentials
OPENAI_API_KEY=replace-me

# For OpenAI itself, you can omit OPENAI_BASE_URL.
# For OpenAI-compatible providers, this is often required and typically ends with /v1.
# Examples:
#   OPENAI_BASE_URL=https://api.openai.com/v1
#   OPENAI_BASE_URL=http://localhost:8080/v1
OPENAI_BASE_URL=

# Model used by router LLM (and also used as codegen fallback default)
OPENAI_MODEL=gpt-4o-mini

# Codegen pipeline models
# If unset, codegen falls back to OPENAI_MODEL (then gpt-4o-mini).
LLM_MODEL_CODEGEN=gpt-4o-mini
LLM_MODEL_REPORTER=gpt-4o-mini

############################################
# Frontend (Vite)
############################################

# Backend base URL used by the UI
VITE_API_BASE_URL=http://127.0.0.1:8000

# Optional: force routing to a specific agent id (e.g. "codegen" or "projplan")
# When set, UI sends { mode: "forced", forced_agent_id: VITE_FORCE_AGENT_ID }
# to /api/v1/router/execute.
VITE_FORCE_AGENT_ID=
